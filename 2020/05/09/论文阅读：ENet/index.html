<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>论文阅读：ENet | MatthewY's Blog</title><meta name="description" content="论文阅读：ENet"><meta name="keywords" content="论文,神经网络"><meta name="author" content="Matthew Yue"><meta name="copyright" content="Matthew Yue"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="论文阅读：ENet"><meta name="twitter:description" content="论文阅读：ENet"><meta name="twitter:image" content="https://ninghaiywx.github.io/img/post.jpeg"><meta property="og:type" content="article"><meta property="og:title" content="论文阅读：ENet"><meta property="og:url" content="https://ninghaiywx.github.io/2020/05/09/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AENet/"><meta property="og:site_name" content="MatthewY's Blog"><meta property="og:description" content="论文阅读：ENet"><meta property="og:image" content="https://ninghaiywx.github.io/img/post.jpeg"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode = '1'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  var isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  var isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  var isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="https://ninghaiywx.github.io/2020/05/09/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AENet/"><link rel="next" title="论文阅读：deeplabv3+" href="https://ninghaiywx.github.io/2020/04/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9Adeeplabv3+/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: 'false',
  isFontAwesomeV5: false,
  isPhotoFigcaption: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isSidebar: true  
  }</script><noscript><style>
#page-header {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="MatthewY's Blog" type="application/atom+xml">
</head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">7</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">3</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">8</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#引言"><span class="toc-number">1.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#网络结构介绍"><span class="toc-number">2.</span> <span class="toc-text">网络结构介绍</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#网络结构设计中的选择"><span class="toc-number">3.</span> <span class="toc-text">网络结构设计中的选择</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Feature-map-resolution"><span class="toc-number">3.1.</span> <span class="toc-text">Feature map resolution</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Early-downsampling"><span class="toc-number">3.2.</span> <span class="toc-text">Early downsampling</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Decoder-size"><span class="toc-number">3.3.</span> <span class="toc-text">Decoder size</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Nonlinear-operations"><span class="toc-number">3.4.</span> <span class="toc-text">Nonlinear operations</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Information-preserving-dimensionality-changes"><span class="toc-number">3.5.</span> <span class="toc-text">Information-preserving dimensionality changes</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Factorizing-filters"><span class="toc-number">3.6.</span> <span class="toc-text">Factorizing filters</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dilated-convolutions"><span class="toc-number">3.7.</span> <span class="toc-text">Dilated convolutions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Regularization"><span class="toc-number">3.8.</span> <span class="toc-text">Regularization</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#实验"><span class="toc-number">4.</span> <span class="toc-text">实验</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#我的实验"><span class="toc-number">5.</span> <span class="toc-text">我的实验</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#下采样bottleneck"><span class="toc-number">5.1.</span> <span class="toc-text">下采样bottleneck</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#上采样bottle"><span class="toc-number">5.2.</span> <span class="toc-text">上采样bottle</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#initial"><span class="toc-number">5.3.</span> <span class="toc-text">initial</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#主体网络结构"><span class="toc-number">5.4.</span> <span class="toc-text">主体网络结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#结果图"><span class="toc-number">5.5.</span> <span class="toc-text">结果图</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(/img/post.jpeg)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">MatthewY's Blog</a></span><span class="pull_right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">论文阅读：ENet</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-05-09 19:01:27"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2020-05-09</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-05-09 21:43:31"><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-05-09</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="fa fa-angle-right post-meta__separator" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%BA%E6%96%87/">论文</a><i class="fa fa-angle-right post-meta__separator" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%BA%E6%96%87/2016%E5%B9%B4/">2016年</a></span></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-comment-o" aria-hidden="true"></i><span>评论数:</span><a href="/2020/05/09/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AENet/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count comment-count" data-xid="/2020/05/09/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AENet/" itemprop="commentCount"></span></a></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>如语义分割之类的像素级别的分类任务已经有了很多的发展，现有的模型都在朝着越来越高的精度发展。但是这样的任务对于嵌入式设备也有着重要的应用，所以做到实时语义分割就是必不可少的环节，但是大多数模型都是朝着高精度去的，在实时性上非常的差。基于实时语义分割的问题，作者提出了一种轻量级的网络结构，有着非常少的参数量可以快速进行语义分割，并且在性能上并不会损失的太多。本文作者是<strong>Abhishek Chaurasia, Sangpil Kim, Eugenio Culurciello</strong></p>
<p><a href="https://arxiv.org/pdf/1606.02147.pdf" target="_blank" rel="noopener">原论文</a></p>
<h1 id="网络结构介绍"><a href="#网络结构介绍" class="headerlink" title="网络结构介绍"></a>网络结构介绍</h1><p>首先先给出ENet的整个网络的结构，论文中是以$512\times512$的图片作为的输入<img src="/" class="lazyload" data-src="https://s1.ax1x.com/2020/05/09/YlW9II.png"  alt="YlW9II.png"></p>
<p>可以从图中看出网络也采用了类似U型的结构，只不过是非对称的，下采样有5个阶段，下采样了8倍，上采样只有2个阶段。</p>
<p>整个网络核心的就是bottleneck与第一步的initial结构。<img src="/" class="lazyload" data-src="https://s1.ax1x.com/2020/05/09/YlfY1f.png"  alt="YlfY1f.png"></p>
<p>上图(a)就是Initial结构，把原图分别用步长卷积和最大池化下采样，再拼接起来进行卷积可以更好的学习得到2倍下采样的结果。同时一般网络结构并不会上来直接进行下采样而是要先进行卷积，但是这样会使得网络更大且有冗余信息，经过实验，在第一层进行下采样不会对最后的分类结果产生影响。</p>
<p>上图(b)就是网络中最为核心的结构，借鉴了ResNet的思想，称之为bottleneck就是因为主分支上先通过$1\times1$卷积减少了通道数，最后再恢复到输出通道数，是一种中间小两边大的结构，就像瓶颈一样，下面介绍下采样的bottleneck的细节。</p>
<p>主分支：</p>
<ul>
<li>先通过$1\times1$卷积进行投影并减少通道数，如果是下采样类型的bottleneck，可在此使用带步长的$1\times1$卷积。</li>
<li>利用一层conv卷积层让模型有更强的学习力，这层conv有不同类型，通过之前网络架构图可以看出。type那一列标注有dilated的就是进行空洞卷积以获取更大的感受野，标注有asymmetric的先进行$1\times5$卷积再进行$5\times1$卷积，这有助于减少参数量，没有标注的就进行普通的$3\times3$卷积。</li>
<li>利用$1\times1$卷积进行投影输出特定通道数的特征图。</li>
<li>为了防止模型出现过拟合现象，再添加一层空间dropout层，在bottleneck2.0之前dropout参数为0.01，之后为0.1。</li>
</ul>
<p>分支：</p>
<ul>
<li>如果bottleneck不是下采样的，则此分支不作任何操作直接与主分支跳跃连接，即与主分支相加。</li>
<li>如果是下采样的则先通过最大池化下采样，再利用padding填充至于主分支特征图大小一致，最后进行相加操作。</li>
</ul>
<p>以上的每个卷积操作后面都紧接BatchNormalization层和PReLU激活函数（作者使用此激活函数而不是ReLU的理由再后面会阐述）。对于$1\times1$卷积，作者都去掉了bias。</p>
<p>上采样类型的bottleneck结构完全相同的只不过把下采样操作换成上采样操作（可以用插值或者转置卷积）。</p>
<h1 id="网络结构设计中的选择"><a href="#网络结构设计中的选择" class="headerlink" title="网络结构设计中的选择"></a>网络结构设计中的选择</h1><h2 id="Feature-map-resolution"><a href="#Feature-map-resolution" class="headerlink" title="Feature map resolution"></a>Feature map resolution</h2><p>下采样有两个缺点：</p>
<ul>
<li>下采样降低了特征图的分辨率，只保留了主要特征，这意味着丢失了一些空间的信息，例如边缘信息这样的细节信息。</li>
<li>全像素的语义分割要求输出图的分辨率与输入图一样，这就要求上采样模块必须十分强大才能很好的还原图像的信息，这意味更大的模型和计算量。</li>
</ul>
<p>于是ENet采用了类似SegNet的方法进行上采样，可以减少内存需求量。同时强力的下采样会损失精度，于是我们尽可能的限制下采样。</p>
<p>但与此同时下采样也带来一定的好处，那就是可以获取更大的感受野，有利于分辨不同类别的物体。于是为了增大感受野，我们在一些卷积中使用了空洞卷积。</p>
<h2 id="Early-downsampling"><a href="#Early-downsampling" class="headerlink" title="Early downsampling"></a>Early downsampling</h2><p>处理高分辨率的图像会消耗大量的计算资源，为了减少模型的复杂程度，我们在第一步就对图像进行了下采样操作。这样做是考虑到视觉信息在空间上是高度冗余的。我们认为最初的网络层更主要的是对特征进行提取，它并不直接有助于分类，因此过早下采样不会产生影响。最终，这样的思想也在实验中得到了很好的验证。</p>
<h2 id="Decoder-size"><a href="#Decoder-size" class="headerlink" title="Decoder size"></a>Decoder size</h2><p>相比于SegNet中编码器和解码器的完全对称，ENet则用了一个较大的编码器和较小的解码器。作者认为Encoder主要进行信息处理和过滤，和流行的分类模型相似。而decoder主要是对encoder的输出做上采样，对细节做细微调整（我认为可能作者也是为了减少模型参数量，毕竟更多的解码器还是有助于信息的恢复的）。</p>
<h2 id="Nonlinear-operations"><a href="#Nonlinear-operations" class="headerlink" title="Nonlinear operations"></a>Nonlinear operations</h2><p>通常在卷积层之前做ReLU和Batch Norm效果会更好，但是在ENet上使用ReLU却降低了精度。相反，我们发现删除网络初始层中的大多数ReLU可以改善结果。所以最终使用了PReLU来代替ReLU，它带来了一个新参数，目的在于学习负数区域的斜率。<img src="/" class="lazyload" data-src="https://s1.ax1x.com/2020/05/09/Ylbe39.png"  alt="Ylbe39.png"></p>
<p>上图展示了PReLU中的参数的变化情况，蓝色的线代表均值，灰色的区域上界代表最大值，下界代表最小值。如果参数为0，则说明用ReLU函数更可取。开始的initial部分方差比较大，波动很剧烈。同时也可以看出在bottleneck部分，参数值更趋向于负值（即中间向下波动的部分），这也解释了为什么ReLU函数不能很好工作的原因。作者认为在bottleneck中ReLU不好用的原因是网络层数太浅，在ResNet中是有数百层的网络的。值得注意的是，解码器的权重变得偏向正数，学习到的函数功能更接近identity。这证实了我们的直觉，即解码器只用于微调上采样输出.</p>
<h2 id="Information-preserving-dimensionality-changes"><a href="#Information-preserving-dimensionality-changes" class="headerlink" title="Information-preserving dimensionality changes"></a>Information-preserving dimensionality changes</h2><p>如前所述，尽早对输入进行降采样十分有必要，但过于激进的降维也会阻碍信息的流动。在Inception V3中提出了解决这一问题的一种非常好的办法。他们认为VGG使用的池化再卷积的扩展维数方式，尽管并不是十分明显，但却引入了代表性瓶颈(导致需要使用更多的filters，降低了计算效率)。<br>另一方面，卷积后的拼接增加了特征映射的深度(increases feature map depth)，消耗了大量计算量。因此，正如在上文中所建议的，我们选择在使用步长2的卷积的同时并行执行池化操作，并将得到的特征图拼接(concatenate)起来。这种技术使我们可以将初始块的推理时间提高10倍。</p>
<p>此外，我们在原始ResNet架构中发现了一个问题。下采样时，卷积分支中的第一个1×1卷积在两个维度上以2的步长滑动，直接丢弃了75%的输入。</p>
<p>ENet将卷积核的大小增加到了2×2，这样可以让整个输入都参与下采样，从而提高信息流和精度。虽然这使得这些层的计算成本增加了4倍，但是在ENET中这些层的数量很少，开销并不明显。</p>
<h2 id="Factorizing-filters"><a href="#Factorizing-filters" class="headerlink" title="Factorizing filters"></a>Factorizing filters</h2><p>卷积权重具有相当大的冗余度，并且每个n×n卷积可以被分解为彼此相继的两个较小的卷积，一个n×1和一个1×n，称为非对称卷积。我们在网络中使用了n= 5的非对称卷积，这两个操作的计算成本类似于单个3×3的卷积。增加了模块的学习功能并增加了感受野。</p>
<p>更重要的是，bottleneck模块中使用的一系列操作（投影，卷积，投影）可以看作是将一个大的卷积层分解为一系列更小更简单的操作，即它的低秩近似。这种因子分解大大的加速了计算速度，并减少了参数的数量，使它们更少冗余。此外，由于在层之间插入的非线性操作，功能也变的更丰富了。</p>
<h2 id="Dilated-convolutions"><a href="#Dilated-convolutions" class="headerlink" title="Dilated convolutions"></a>Dilated convolutions</h2><p>空洞卷积，可以增大感受野并更好的进行分类任务。</p>
<h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p>大多数语义分割数据集图像数量较少，为了防止出现过拟合的现象，加入了正则化层。最开始使用了L2正则化发现效果并不好，最终选取了空间dropout层。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>作者实验部分可以参看<a href="https://arxiv.org/pdf/1606.02147.pdf" target="_blank" rel="noopener">原论文</a>，可以看出相比其它一些大型网络，fps有非常显著的提升，分类精度的下降也可以接受，只在一些边缘地方能看出来比较大的瑕疵，下面就是论文中的一张实验结果对比图。</p>
<p><img src="/" class="lazyload" data-src="https://s1.ax1x.com/2020/05/09/YlOdD1.png"  alt="YlOdD1.png"></p>
<p>能看在边缘以及小物体的分类上有比较多的瑕疵。</p>
<h1 id="我的实验"><a href="#我的实验" class="headerlink" title="我的实验"></a>我的实验</h1><p>实验使用的tensorflow2.0，在google colab上跑了<a href="https://drive.google.com/file/d/0B0d9ZiqAgFkiOHR1NTJhWVJMNEU/view" target="_blank" rel="noopener">此数据集</a>。</p>
<h2 id="下采样bottleneck"><a href="#下采样bottleneck" class="headerlink" title="下采样bottleneck"></a>下采样bottleneck</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bottleneck</span><span class="params">(x,output,s,methods,dropout_rate=<span class="number">0.1</span>,scale=<span class="number">4</span>,asymmetric=<span class="number">5</span>,d_rate=<span class="number">5</span>)</span>:</span></span><br><span class="line">  temp = output//scale</span><br><span class="line">  </span><br><span class="line">  x_residual = x</span><br><span class="line">  x_residual = Conv2D(temp,(<span class="number">1</span>,<span class="number">1</span>),(s,s),padding=<span class="string">'same'</span>,use_bias=<span class="literal">False</span>)(x_residual)</span><br><span class="line">  x_residual = BatchNormalization()(x_residual)</span><br><span class="line">  x_residual = PReLU(shared_axes=[<span class="number">1</span>,<span class="number">2</span>])(x_residual)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> methods == <span class="string">'norm'</span>:</span><br><span class="line">    x_residual = Conv2D(temp,(<span class="number">3</span>,<span class="number">3</span>),padding=<span class="string">'same'</span>)(x_residual)</span><br><span class="line">  <span class="keyword">elif</span> methods == <span class="string">'dilated'</span>:</span><br><span class="line">    x_residual = Conv2D(temp,(<span class="number">3</span>,<span class="number">3</span>),padding=<span class="string">'same'</span>,dilation_rate=d_rate)(x_residual)</span><br><span class="line">  <span class="keyword">elif</span> methods == <span class="string">'asymmetric'</span>:</span><br><span class="line">    x_residual = Conv2D(temp,(<span class="number">1</span>,asymmetric),padding=<span class="string">'same'</span>,dilation_rate=d_rate)(x_residual)</span><br><span class="line">    x_residual = Conv2D(temp,(asymmetric,<span class="number">1</span>),padding=<span class="string">'same'</span>,dilation_rate=d_rate)(x_residual)</span><br><span class="line">  x_residual = BatchNormalization()(x_residual)</span><br><span class="line">  x_residual = PReLU(shared_axes=[<span class="number">1</span>,<span class="number">2</span>])(x_residual)</span><br><span class="line"></span><br><span class="line">  x_residual = Conv2D(output,(<span class="number">1</span>,<span class="number">1</span>),use_bias=<span class="literal">False</span>)(x_residual)</span><br><span class="line">  x_residual = SpatialDropout2D(rate=dropout_rate)(x_residual)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> s == <span class="number">2</span>:</span><br><span class="line">    x = MaxPool2D(padding=<span class="string">'same'</span>)(x)</span><br><span class="line">  x = Conv2D(output,(<span class="number">1</span>,<span class="number">1</span>),use_bias=<span class="literal">False</span>)(x)</span><br><span class="line">  x = Add()([x,x_residual])</span><br><span class="line">  x = BatchNormalization()(x)</span><br><span class="line">  x = PReLU(shared_axes=[<span class="number">1</span>,<span class="number">2</span>])(x)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h2 id="上采样bottle"><a href="#上采样bottle" class="headerlink" title="上采样bottle"></a>上采样bottle</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">de_bottleneck</span><span class="params">(x,output,s,dropout_rate=<span class="number">0.1</span>,scale=<span class="number">4</span>)</span>:</span></span><br><span class="line">  temp = output//scale</span><br><span class="line"></span><br><span class="line">  x_residual = x</span><br><span class="line">  x_residual = Conv2D(temp,(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">'same'</span>,use_bias=<span class="literal">False</span>)(x_residual)</span><br><span class="line">  x_residual = BatchNormalization()(x_residual)</span><br><span class="line">  x_residual = PReLU(shared_axes=[<span class="number">1</span>,<span class="number">2</span>])(x_residual)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> s == <span class="number">2</span>:</span><br><span class="line">    x_residual = Conv2DTranspose(temp,(<span class="number">3</span>,<span class="number">3</span>),(s,s),padding=<span class="string">'same'</span>)(x_residual)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    x_residual = Conv2D(temp,(<span class="number">3</span>,<span class="number">3</span>),padding=<span class="string">'same'</span>)(x_residual)</span><br><span class="line">  x_residual = BatchNormalization()(x_residual)</span><br><span class="line">  x_residual = PReLU(shared_axes=[<span class="number">1</span>,<span class="number">2</span>])(x_residual)</span><br><span class="line"></span><br><span class="line">  x_residual = Conv2D(output,(<span class="number">1</span>,<span class="number">1</span>),use_bias=<span class="literal">False</span>)(x_residual)</span><br><span class="line">  x_residual = SpatialDropout2D(rate=dropout_rate)(x_residual)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> s == <span class="number">2</span>:</span><br><span class="line">    x = UpSampling2D((s,s),interpolation=<span class="string">'bilinear'</span>)(x)</span><br><span class="line">  x = Conv2D(output,(<span class="number">1</span>,<span class="number">1</span>),use_bias=<span class="literal">False</span>)(x)</span><br><span class="line">  x = Add()([x,x_residual])</span><br><span class="line">  x = BatchNormalization()(x)</span><br><span class="line">  x = PReLU(shared_axes=[<span class="number">1</span>,<span class="number">2</span>])(x)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h2 id="initial"><a href="#initial" class="headerlink" title="initial"></a>initial</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initial</span><span class="params">(x)</span>:</span></span><br><span class="line">  x_1 = x</span><br><span class="line">  x = MaxPool2D(padding=<span class="string">'same'</span>)(x)</span><br><span class="line">  x_1 = Conv2D(<span class="number">13</span>,(<span class="number">3</span>,<span class="number">3</span>),(<span class="number">2</span>,<span class="number">2</span>),padding=<span class="string">'same'</span>)(x_1)</span><br><span class="line"></span><br><span class="line">  x = Concatenate()([x,x_1])</span><br><span class="line">  <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h2 id="主体网络结构"><a href="#主体网络结构" class="headerlink" title="主体网络结构"></a>主体网络结构</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ENet</span><span class="params">(input_shape,n_class)</span>:</span></span><br><span class="line">  x_input = Input(input_shape)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#encoder</span></span><br><span class="line">  x_1 = initial(x_input)</span><br><span class="line">  x = bottleneck(x_1,<span class="number">64</span>,<span class="number">2</span>,<span class="string">'norm'</span>,dropout_rate=<span class="number">0.01</span>)</span><br><span class="line">  x = bottleneck(x,<span class="number">64</span>,<span class="number">1</span>,<span class="string">'norm'</span>,dropout_rate=<span class="number">0.01</span>)</span><br><span class="line">  x = bottleneck(x,<span class="number">64</span>,<span class="number">1</span>,<span class="string">'norm'</span>,dropout_rate=<span class="number">0.01</span>)</span><br><span class="line">  x = bottleneck(x,<span class="number">64</span>,<span class="number">1</span>,<span class="string">'norm'</span>,dropout_rate=<span class="number">0.01</span>)</span><br><span class="line">  x_2 = bottleneck(x,<span class="number">64</span>,<span class="number">1</span>,<span class="string">'norm'</span>,dropout_rate=<span class="number">0.01</span>)</span><br><span class="line">  </span><br><span class="line">  x = bottleneck(x_2,<span class="number">128</span>,<span class="number">2</span>,<span class="string">'norm'</span>)</span><br><span class="line">  x = bottleneck(x,<span class="number">128</span>,<span class="number">1</span>,<span class="string">'norm'</span>)</span><br><span class="line">  x = bottleneck(x,<span class="number">128</span>,<span class="number">1</span>,<span class="string">'dilated'</span>,d_rate=<span class="number">2</span>)</span><br><span class="line">  x = bottleneck(x,<span class="number">128</span>,<span class="number">1</span>,<span class="string">'asymmetric'</span>)</span><br><span class="line">  x = bottleneck(x,<span class="number">128</span>,<span class="number">1</span>,<span class="string">'dilated'</span>,d_rate=<span class="number">4</span>)</span><br><span class="line">  x = bottleneck(x,<span class="number">128</span>,<span class="number">1</span>,<span class="string">'norm'</span>)</span><br><span class="line">  x = bottleneck(x,<span class="number">128</span>,<span class="number">1</span>,<span class="string">'dilated'</span>,d_rate=<span class="number">8</span>)</span><br><span class="line">  x = bottleneck(x,<span class="number">128</span>,<span class="number">1</span>,<span class="string">'asymmetric'</span>)</span><br><span class="line">  x = bottleneck(x,<span class="number">128</span>,<span class="number">1</span>,<span class="string">'dilated'</span>,d_rate=<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">  x = bottleneck(x,<span class="number">128</span>,<span class="number">1</span>,<span class="string">'norm'</span>)</span><br><span class="line">  x = bottleneck(x,<span class="number">128</span>,<span class="number">1</span>,<span class="string">'dilated'</span>,d_rate=<span class="number">2</span>)</span><br><span class="line">  x = bottleneck(x,<span class="number">128</span>,<span class="number">1</span>,<span class="string">'asymmetric'</span>)</span><br><span class="line">  x = bottleneck(x,<span class="number">128</span>,<span class="number">1</span>,<span class="string">'dilated'</span>,d_rate=<span class="number">4</span>)</span><br><span class="line">  x = bottleneck(x,<span class="number">128</span>,<span class="number">1</span>,<span class="string">'norm'</span>)</span><br><span class="line">  x = bottleneck(x,<span class="number">128</span>,<span class="number">1</span>,<span class="string">'dilated'</span>,d_rate=<span class="number">8</span>)</span><br><span class="line">  x = bottleneck(x,<span class="number">128</span>,<span class="number">1</span>,<span class="string">'asymmetric'</span>)</span><br><span class="line">  x = bottleneck(x,<span class="number">128</span>,<span class="number">1</span>,<span class="string">'dilated'</span>,d_rate=<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#decoder</span></span><br><span class="line">  x = de_bottleneck(x,<span class="number">64</span>,<span class="number">2</span>)</span><br><span class="line">  x = Concatenate()([x,x_2])</span><br><span class="line">  x = Conv2D(<span class="number">64</span>,(<span class="number">3</span>,<span class="number">3</span>),padding=<span class="string">'same'</span>)(x)</span><br><span class="line">  x = BatchNormalization()(x)</span><br><span class="line">  x = PReLU(shared_axes=[<span class="number">1</span>,<span class="number">2</span>])(x)</span><br><span class="line">  x = de_bottleneck(x,<span class="number">64</span>,<span class="number">1</span>)</span><br><span class="line">  x = de_bottleneck(x,<span class="number">64</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  x = de_bottleneck(x,<span class="number">16</span>,<span class="number">2</span>)</span><br><span class="line">  x = Concatenate()([x,x_1])</span><br><span class="line">  x = Conv2D(<span class="number">16</span>,(<span class="number">3</span>,<span class="number">3</span>),padding=<span class="string">'same'</span>)(x)</span><br><span class="line">  x = BatchNormalization()(x)</span><br><span class="line">  x = PReLU(shared_axes=[<span class="number">1</span>,<span class="number">2</span>])(x)</span><br><span class="line">  x = de_bottleneck(x,<span class="number">16</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  x = Conv2DTranspose(n_class,(<span class="number">4</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">2</span>),padding=<span class="string">'same'</span>)(x)</span><br><span class="line">  x = Activation(<span class="string">'softmax'</span>)(x)</span><br><span class="line"></span><br><span class="line">  model = keras.Model(x_input,x)</span><br><span class="line">  <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>

<h2 id="结果图"><a href="#结果图" class="headerlink" title="结果图"></a>结果图</h2><p><img src="/" class="lazyload" data-src="https://s1.ax1x.com/2020/05/09/YljlfU.png"  alt="YljlfU.png"></p>
<p>最终的训练结果在训练集上到达91%的准确率，相比大型的网络模型要低不少，在小物体和边缘方面存在不精确的地方，肉眼就能看得出。但是它的推理时间则快了很多。训练一个epoch的时间仅有大型网络的$\frac{1}{3}-\frac{1}{7}$.</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Matthew Yue</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://ninghaiywx.github.io/2020/05/09/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AENet/">https://ninghaiywx.github.io/2020/05/09/论文阅读：ENet/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://ninghaiywx.github.io" target="_blank">MatthewY's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87/">论文</a><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a></div><div class="post_share"><div class="social-share" data-image="/img/post.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2020/04/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9Adeeplabv3+/"><img class="next_cover lazyload" data-src="/img/post.jpeg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">论文阅读：deeplabv3+</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/04/01/论文阅读：AdderNet：Do We Really Need Multiplications in Deep Learning?/" title="论文阅读：AdderNet：Do We Really Need Multiplications in Deep Learning？"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpeg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-01</div><div class="relatedPosts_title">论文阅读：AdderNet：Do We Really Need Multiplications in Deep Learning？</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/10/论文阅读：DCGAN/" title="论文阅读：DCGAN网络"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpeg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-10</div><div class="relatedPosts_title">论文阅读：DCGAN网络</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/17/论文阅读：u-net/" title="论文阅读：u-net"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpeg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-17</div><div class="relatedPosts_title">论文阅读：u-net</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/论文阅读：Generative Adversarial Nets/" title="论文阅读：Generative Adversarial Nets"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpeg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">论文阅读：Generative Adversarial Nets</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/24/论文阅读：deeplabv3+/" title="论文阅读：deeplabv3+"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpeg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-24</div><div class="relatedPosts_title">论文阅读：deeplabv3+</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;

window.valine = new Valine({
  el:'#vcomment',
  notify: true,
  verify: false,
  appId: 'seo2H5IJVc1KgAHwVS5HSaPTC-gzGzoHsz',
  appKey: 'OEKe6VoQK7MawafzBxXRY4Ya',
  placeholder: '留下你的评论吧~',
  avatar: 'monsterid',
  meta: guest_info,
  pageSize: '10',
  lang: 'zh-cn',
  recordIP: false,
  serverURLs: ''
});</script></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By Matthew Yue</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="/js/third-party/click_heart.js"></script><script src="/js/search/local-search.js"></script></body></html>