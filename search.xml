<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Gan网络全局最优解推导</title>
    <url>/2020/04/18/Gan%E7%BD%91%E7%BB%9C%E5%85%A8%E5%B1%80%E6%9C%80%E4%BC%98%E8%A7%A3%E6%8E%A8%E5%AF%BC/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>本文是对GAN网络中目标函数全局最优解情况的推导，推导过程在<a href="https://arxiv.org/pdf/1406.2661.pdf" target="_blank" rel="noopener">原论文</a>有给出。这里再对其进行一个推导。</p>
<h1 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h1><p>首先重新看一下目标函数$$\underset Gmin\underset Dmax V(D,G)=E_{x_\tilde{} p_{data}}(x)[logD(x)]+E_{z\tilde{} p_z(z)}[log(1-D(G(z)))]$$</p>
<p>我们可以先固定G，来求一下D在什么情况下使得函数最大。就变成了求$$\underset Dmax V(D,G)$$</p>
<h2 id="求解-underset-Dmax-V-D-G"><a href="#求解-underset-Dmax-V-D-G" class="headerlink" title="求解$\underset Dmax V(D,G)$"></a>求解$\underset Dmax V(D,G)$</h2><p>我们先对V(D,G)变一下形，即$$\int_x p_{data}(x)log(D(x))dx+\int_zp_z(z)log(1-D(g(z)))dz$$$$=\int_xp_{data}(x)log(D(x))+p_g(x)log(1-D(x))dx$$</p>
<p>我们知道对于形如$alog(y)+blog(1-y)$的函数它在[0,1]区间内在$\frac{a}{a+b}$取得最大值，区间为[0,1]是因为x表示的概率分布是[0,1]之间的。所以V(D,G)在$D_G^* = \frac{p_{data}(x)}{p_{data}(x)+p_g(x)}$取得最大值。</p>
<p>接下来就是求解$\underset GminV(D,G)$。</p>
<h2 id="求解-underset-GminV-D-G"><a href="#求解-underset-GminV-D-G" class="headerlink" title="求解$\underset GminV(D,G)$"></a>求解$\underset GminV(D,G)$</h2><p>将$\underset DmaxV(D,G)$的$D^* $的值带入原目标函数，则变成了$$C(G) = \underset Dmax V(D,G)$$<br>$$= E_{x_\tilde{} p_{data}}[logD_G^* (x)]+E_{z\tilde{} p_z}[log(1-D_G^* (G(z)))]$$<br>$$=E_{x_\tilde{} p_{data}}[logD_G^* (x)]+E_{x\tilde{} p_g}[log(1-D_G^* (G(x))]$$<br>$$=E_{x_\tilde{} p_{data}}[log\frac{p_{data}(x)}{p_{data}(x)+p_g(x)}]+E_{x\tilde{} p_g}[log\frac{p_g(x)}{p_{data}(x)+p_g(x)}]$$<br>$$=E_{x_\tilde{} p_{data}}[log(\frac{p_{data}(x)}{(p_{data}(x)+p_g(x))/2}* \frac{1}{2})]+E_{x\tilde{} p_g}[log(\frac{p_g(x)}{(p_{data}(x)+p_g(x))/2}* \frac{1}{2})]$$<br>$$=KL(p_{data}||\frac{p_{data}+p_g}{2})+KL(p_g||\frac{p_{data}+p_g}{2})-log4$$<br>由KL散度性质得知当两个分布相等的时候，KL散度为0，所以C(G)在$p^* _g=p_d$的时候最小。把$p^* _g=p_d$再带回$D_G^* $得到$D_G^* =\frac{1}{2}$，这个就是目标函数的全局最优解。</p>
]]></content>
      <categories>
        <category>深度学习</category>
        <category>数学</category>
      </categories>
      <tags>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：AdderNet：Do We Really Need Multiplications in Deep Learning？</title>
    <url>/2020/04/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AAdderNet%EF%BC%9ADo%20We%20Really%20Need%20Multiplications%20in%20Deep%20Learning?/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>如今的卷积神经网络(CNN)的卷积操作中包含了大量的乘法，虽然已经有很多轻量级的网络(如MobileNet)提出来，但是乘法的开销依旧是难以忽视的，要在轻量级设备本地进行深度学习应用，需要使得计算速度进一步加快，于是本论文提出了使用加法操作代替乘法操作，本论文的作者是<strong>Hanting Chen,Yunhe Wang,Chunjing Xu,Boxin Shi,Chao Xu,Qi Tian,Chang Xu</strong>。是CVPR2020上华为诺亚实验室和北京大学合作的一篇文章。<br>论文指出传统卷积操作其实就是使用的一种互相关操作来衡量输入特征和卷积核之间的相似度，而这个互相关的操作就引入了很多乘法操作，因此文章提出另一种方式来衡量输入特征和卷积核之间的相似度，这个方法就是<strong>L1距离</strong>。</p>
<p><a href="https://arxiv.org/pdf/1912.13200.pdf" target="_blank" rel="noopener">原论文</a></p>
<h1 id="没有乘法的网络"><a href="#没有乘法的网络" class="headerlink" title="没有乘法的网络"></a>没有乘法的网络</h1><p>假设$F \in R^{d \times d \times c_in \times c_{out}}$，F是网络中间某层的过滤器，过滤器大小为d，输入有$c_{in}$个通道，输出有$c_{out}$个通道。输入的特征定义为$X \in R^{H \times W \times c_{in}}$，其中H和W对应着特征的高和宽。则输出的Y有以下式子$$Y(m,n,t) = \sum_{i=0}^n \sum_{j=0}^d \sum_{k=0}^{c_{in}}S(X(m+i,n+j,k),F(i,j,k,t))$$其中S是相似度函数。如果$S(x,y) = x \times y$,那么这个公式就变成了传统卷积神经网络中的卷积操作。</p>
<h3 id="加法网络"><a href="#加法网络" class="headerlink" title="加法网络"></a>加法网络</h3><p>前面提到过要用L1距离代替互相关操作，那么上述公式就变成$$Y(m,n,t) = - \sum_{i=0}^n \sum_{j=0}^d \sum_{k=0}^{c_{in}}|X(m+i,n+j,k)-F(i,j,k,t)|$$这里作者提到了这样的操作得出的结果都是负值，但是传统卷积网络得到的输出值则是有正有负，但是在输出层后紧接着一个BN层我们可以让输出分布在一个合理的范围。</p>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><p>在传统卷积网络中Y关于F的反向传播公式如下$$\frac{\partial Y(m,n,t)}{\partial F(i,j,k,t) } = X(m+i,n+j,k)$$在加法网络中，使用了L1距离的反向传播公式如下$$\frac{\partial Y(m,n,t)}{\partial F(i,j,k,t)} = sgn(X(m+i,n+j,k) - F(i,j,k,t))$$文章中说L1距离反向传播的这种signSGD并不能沿着最好的方向下降，有时候还会选择比较糟糕的方向，因此论文中把反向传播公式变成L2距离的反向传播，叫做全精度梯度$$\frac{\partial Y(m,n,t)}{\partial F(i,j,k,t) } = X(m+i,n+j,k) - F(i,j,k,t)$$同时再考虑Y对X的导数，根据链式法则，$\frac{\partial Y}{\partial F_i }$只跟$F_i$自己有关，而$\frac{\partial Y}{\partial X_i }$的梯度值还会影响前一层的值，作者指出这种全精度梯度传播会产生梯度爆炸，于是使用一个HT函数把梯度截断在[-1,1]里，既$$\frac{\partial Y(m,n,t)}{\partial X(m+i,n+j,k) } = HT(F(i,j,k,t) - F(i,j,k,t))$$其中HT代表着HardTanh函数$$HT(x) = x\quad if -1&lt;x&lt;1$$$$HT(x) = 1 \quad x&gt;1$$$$HT(x)=-1\quad x&lt;-1$$</p>
<h2 id="自适应学习率"><a href="#自适应学习率" class="headerlink" title="自适应学习率"></a>自适应学习率</h2><p>在加法网络中，每一层输出后面也跟着一层BN层，虽然BN层带来了一些乘法操作，但是这些操作的量级比起经典卷积网络的乘法数量就可以忽略不计。给定一个mini-batch B={x1,…,xm}，BN层做了如下操作$$y = \gamma \frac{x-\mu_B}{\sigma_B}+\beta$$$$\mu=\frac{1}{m}\sum_ix_i$$$$\sigma_B^2=\frac{1}{m}\sum_i(x_i-\mu_B)^2$$该层主要就是处理这个mini-batch的数据集使它均值等于0，方差等于1。那么加上了BN层后l关于x的偏导为$$\frac{\partial l}{\partial x_i}=\sum_{j=1}^m\frac{\gamma}{m^2\sigma_B}{\frac{\partial l}{\partial y_i}-\frac{\partial l}{\partial y_j}[1+\frac{(x_i-x_j)(x_j-\mu_B)}{\sigma_B}]}$$根据链式法则，每一层的权重的梯度都受到上一层$x_i$梯度的影响，根据上述公式$x_i$的梯度很大程度取决于$\sigma_B$，即BN处理前$x_i$的标准差。论文中给出了粗略的输出Y的方差计算，传统神经网络输出方差为$$Var[Y_{CNN}]=\sum_{i=0}^d \sum_{j=0}^d \sum_{k=0}^{c_{in}}Var[X \times F]=d^2c_{in}Var[X]Var[F]$$而在加法网络中方差变为$$Var[Y_{CNN}]=\sum_{i=0}^d \sum_{j=0}^d \sum_{k=0}^{c_{in}}Var[|X-F|]=(1-\frac{2}{\pi})d^2c_{in}(Var[X]+Var[F])$$根据以往经验，$Var[F]$在普通CNN网络中非常的小，只有$10^{-3}$或者$10^{-4}$。因此传统CNN中$Var[Y]$方差要比加法网络中$Var[Y]$大很多。之前提到权重梯度取决于标准差，因此用了BN层的加法网络中权重梯度会很小，下表是个对比<img src="/" class="lazyload" data-src="https://pic.downk.cc/item/5e858168504f4bcb04cb2208.png"  alt="">图片除显示出梯度较小之外，还展示出有些层可能值不在一个量级，因此使用全局统一的学习率变得不再合适，所以论文中使用了一种自适应学习率的方法，使得学习率在每一层都不一样。它的公式计算表示为$$\Delta F_l = \gamma \times \alpha_l \times \Delta L(F_l)$$其中$\gamma$是全局学习率，而$\alpha_l$则是每层的学习率。$$\alpha_l = \frac{\eta \sqrt k}{\parallel \Delta L(F_l) \parallel _ 2}$$k代表$F_l$中元素的个数，$\eta$则是一个超参数。有了这样的学习率调整，在每层中都能自动适应当层的情况进行学习率的调整。</p>
<h2 id="算法流程描述"><a href="#算法流程描述" class="headerlink" title="算法流程描述"></a>算法流程描述</h2><p>上面就是本篇论文提出的所有新东西与新设计，文章中也给出了加法网络前向和后向传播的流程描述<img src="/" class="lazyload" data-src="https://pic.downk.cc/item/5e858a4f504f4bcb04d21d13.png"  alt=""></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>实验数据和结果对比见<a href="https://arxiv.org/pdf/1912.13200.pdf" target="_blank" rel="noopener">原论文</a></p>
<h1 id="一些想法"><a href="#一些想法" class="headerlink" title="一些想法"></a>一些想法</h1><ol>
<li><p>本论文还只是初步提出了一个idea以及初步进行了实验，实验内容也并不复杂，这个网络的稳定性和各个方面的性能有待进一步研究，但不失为一个非常好的思路。</p>
</li>
<li><p>本论文的目的就是打造一个计算更快的网络，但是实验结果没有给出这个网络所耗费的时间。</p>
</li>
<li><p>论文中有些公式的提出并没有给出特别明确的理由（也许我自己我没理解），如L1距离的公式为什么前面要带负号，以及自适应学习率的公式为什么是那样。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>深度学习</category>
        <category>论文</category>
        <category>2020年</category>
      </categories>
      <tags>
        <tag>论文</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：DCGAN网络</title>
    <url>/2020/04/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADCGAN/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>本篇文论文所介绍的DCGAN网络就是在GAN网络的基础上进行改进并引入了图像的卷积操作，即把CNN与GAN网络很好的结合起来，使得网络的生成器最终可以从随机噪声中生成一张以假乱真的图片。该论文作者<strong>Alec Radford &amp; Luke Metz</strong>。作者在文章中并没有对DCGAN网络的具体原理进行公式性的阐述，而是直接呈现了作者自己设计的网络架构和一些参数调整的细节。相关摘要，介绍和相关工作可以直接查看<a href="https://arxiv.org/pdf/1511.06434.pdf" target="_blank" rel="noopener">原论文</a></p>
<h1 id="方法和模型架构"><a href="#方法和模型架构" class="headerlink" title="方法和模型架构"></a>方法和模型架构</h1><p>具体来说本篇论文就是把GAN原始论文中的生成器G和判别器D用两个CNN网络来替代。对于这两个CNN网络，做出了一些调整。</p>
<p>首先就是使用了一个全卷积网络，用步长卷积(即步长大于1的卷积)来替换掉池化层，这样的目的在于希望网络可以自己学习到下采样的方式，相比于固定的池化层更加灵活。这样的方法同时运用在生成器和判别器中（在生成器中主要是要使用转职卷积进行上采样）。</p>
<p>第二就是当时的趋势是取消全连接层，最近的做法是使用全局平均池化层代替全连接层，但是这样做虽然提高了稳定性却降低了收敛速度。对于生成器本，GAN的输入是采用均匀分布初始化的一维噪声，之后还是会使用一个全连接层，得到的结果reshape成一个4D张量，就可以进行一层层的卷积操作。对于判别器，最后的卷积层则是先flatten然后再送入sigmoid分类器进行输出。</p>
<p>第三就是每层中都加入Batch Normalization层，这有助于模型的稳定和收敛，也有助防止过拟合。但是通过实验表明，对网络所有层使用BN层会使得样本不稳定，所以只对生成器的输出和判别器的输入层加入BN层。</p>
<p>生成器中，除了输出层用Tanh，其余层都是用ReLU。而对于判别器，使用LeakyReLU则要更好点。</p>
<p>综上所述，本网络的一些改进的点在于：</p>
<ul>
<li>将所有的池化层都用步长卷积（判别器）和转置步长卷积（生成器）代替。</li>
<li>在生成器和判别器中加入BN层。</li>
<li>对于深层网络架构去除全连接层。</li>
<li>对生成器的除了输出层的其它层使用ReLU，输出层用Tanh。</li>
<li>对判别器所有曾都是用LeakyReLU。</li>
</ul>
<h1 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h1><p>接下来作者提及了一些他训练模型中运用的一些参数调整。</p>
<ul>
<li>首先图片并没有进行一些预处理，只是利用tanh将它的输出映射在了[-1,1]的区间。</li>
<li>使用mini-batch SGD，batch大小为128。</li>
<li>所有的参数都采用0均值,标准差为0.02的初始化方式。</li>
<li>LeakyReLU的斜率设置为0.2。</li>
<li>使用Adam优化器，由于推荐学习率0.001过大，作者修改到了使用0.0002。此外将$\beta_1$的值从默认的0.9调整到了0.5保证模型的稳定。</li>
</ul>
<p>下图是作者给出的生成器的架构图。<img src="/" class="lazyload" data-src="https://pic.downk.cc/item/5e9066a4504f4bcb04a9edca.png"  alt="">第一层就是一个均匀分布的一维噪声，然后经过一层后映射成$4<em>4</em>1024$的特征图。其中的上采样过程使用了转置卷积，最后的输出就是本次模型生成的图片大小，接下来它将会交给判别器进行判别。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>DCGAN的网络实现在github上已经有很多现成的实现了，这里使用了tensorflow官网教程中关于DCGAN的实现。在具体应用中可以发现，其实许多的参数和网络结构设置都是可以根据实际实验需要进行调整的，像在tensorflow的官网教程中，对生成器和判别器中激活函数都有些许调整并且使用了全连接层以及加入了dropout层。</p>
<p>下面就是生成器的代码架构：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_generator_model</span><span class="params">()</span>:</span></span><br><span class="line">    model = tf.keras.Sequential()</span><br><span class="line">    model.add(layers.Dense(<span class="number">7</span>*<span class="number">7</span>*<span class="number">256</span>, use_bias=<span class="literal">False</span>, input_shape=(<span class="number">100</span>,)))</span><br><span class="line">    model.add(layers.BatchNormalization())</span><br><span class="line">    model.add(layers.LeakyReLU())</span><br><span class="line"></span><br><span class="line">    model.add(layers.Reshape((<span class="number">7</span>, <span class="number">7</span>, <span class="number">256</span>)))</span><br><span class="line">    <span class="keyword">assert</span> model.output_shape == (<span class="literal">None</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">256</span>) <span class="comment"># 注意：batch size 没有限制</span></span><br><span class="line"></span><br><span class="line">    model.add(layers.Conv2DTranspose(<span class="number">128</span>, (<span class="number">5</span>, <span class="number">5</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'same'</span>, use_bias=<span class="literal">False</span>))</span><br><span class="line">    <span class="keyword">assert</span> model.output_shape == (<span class="literal">None</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">128</span>)</span><br><span class="line">    model.add(layers.BatchNormalization())</span><br><span class="line">    model.add(layers.LeakyReLU())</span><br><span class="line"></span><br><span class="line">    model.add(layers.Conv2DTranspose(<span class="number">64</span>, (<span class="number">5</span>, <span class="number">5</span>), strides=(<span class="number">2</span>, <span class="number">2</span>), padding=<span class="string">'same'</span>, use_bias=<span class="literal">False</span>))</span><br><span class="line">    <span class="keyword">assert</span> model.output_shape == (<span class="literal">None</span>, <span class="number">14</span>, <span class="number">14</span>, <span class="number">64</span>)</span><br><span class="line">    model.add(layers.BatchNormalization())</span><br><span class="line">    model.add(layers.LeakyReLU())</span><br><span class="line"></span><br><span class="line">    model.add(layers.Conv2DTranspose(<span class="number">1</span>, (<span class="number">5</span>, <span class="number">5</span>), strides=(<span class="number">2</span>, <span class="number">2</span>), padding=<span class="string">'same'</span>, use_bias=<span class="literal">False</span>, activation=<span class="string">'tanh'</span>))</span><br><span class="line">    <span class="keyword">assert</span> model.output_shape == (<span class="literal">None</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>

<p>可以看到第一层用了全连接，并且中间层也并没有使用原论文中所说的ReLU而是也用了LeakyReLU。</p>
<p>而判别器的代码则是：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_discriminator_model</span><span class="params">()</span>:</span></span><br><span class="line">    model = tf.keras.Sequential()</span><br><span class="line">    model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">5</span>, <span class="number">5</span>), strides=(<span class="number">2</span>, <span class="number">2</span>), padding=<span class="string">'same'</span>,</span><br><span class="line">                                     input_shape=[<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>]))</span><br><span class="line">    model.add(layers.LeakyReLU())</span><br><span class="line">    model.add(layers.Dropout(<span class="number">0.3</span>))</span><br><span class="line"></span><br><span class="line">    model.add(layers.Conv2D(<span class="number">128</span>, (<span class="number">5</span>, <span class="number">5</span>), strides=(<span class="number">2</span>, <span class="number">2</span>), padding=<span class="string">'same'</span>))</span><br><span class="line">    model.add(layers.LeakyReLU())</span><br><span class="line">    model.add(layers.Dropout(<span class="number">0.3</span>))</span><br><span class="line"></span><br><span class="line">    model.add(layers.Flatten())</span><br><span class="line">    model.add(layers.Dense(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>

<p>加入了dropout层。</p>
<p>损失函数的定义也是根据原始GAN网络来定义的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discriminator_loss</span><span class="params">(real_output, fake_output)</span>:</span></span><br><span class="line">    real_loss = cross_entropy(tf.ones_like(real_output), real_output)</span><br><span class="line">    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)</span><br><span class="line">    total_loss = real_loss + fake_loss</span><br><span class="line">    <span class="keyword">return</span> total_loss</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator_loss</span><span class="params">(fake_output)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> cross_entropy(tf.ones_like(fake_output), fake_output)</span><br></pre></td></tr></table></figure>

<p>具体的教程可以去<a href="https://www.tensorflow.org/tutorials/generative/dcgan" target="_blank" rel="noopener">tensorflow官网教程</a>进行查阅，它实现了一个自动生成手写数据集的网络，在这个框架的基础上进行改动也可以生成各种其它尺寸的不同图片。我对网络结构修改后在动漫人物头像的数据集上训练后，网络便能生成动漫人物头像，大概训练了200步，可以看到还是有不少瑕疵。</p>
<p><img src="/" class="lazyload" data-src="https://pic.downk.cc/item/5e906f11504f4bcb04b1a9e6.png"  alt=""></p>
<p>头像数据集的获取可以点击查看<a href="https://www.cnblogs.com/baiting/p/8314936.html" target="_blank" rel="noopener">这篇博客</a></p>
<p>实际实验中，使用反卷积的时候，当步长不能整除卷积核大小的时候，上采样出来的图像会很容易出现棋盘效应，棋盘效应的直观原因可以参考<a href="https://www.cnblogs.com/hellcat/p/9707204.html" target="_blank" rel="noopener">这里</a>，应该说转置卷积不可避免的就会带来棋盘效应，就算步长能整除卷积核大小，也可能因为权重分学习的不均匀导致棋盘效应。所以推荐的做法是使用插值先进行上采样再用same卷积也能达到同样的效果且没有棋盘效应。</p>
<p>该篇论文给出CNN和GAN结合的做法并进行了改进，在实际使用中，也可以根据实际不同的需求进行调整以达到最优的情况。</p>
]]></content>
      <categories>
        <category>深度学习</category>
        <category>论文</category>
        <category>2016年</category>
      </categories>
      <tags>
        <tag>论文</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：Generative Adversarial Nets</title>
    <url>/2020/04/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AGenerative%20Adversarial%20Nets/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>这篇论文是生成对抗(GAN)网络的奠基之作，它提出了一种全新生成的思想，时至今日都是深度学习中热门的一种模型。它的作者是<strong>Ian J.Goodfellow, Jean Pouget-Abadie*, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio</strong></p>
<p><a href="https://arxiv.org/pdf/1406.2661.pdf" target="_blank" rel="noopener">原论文</a></p>
<h1 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h1><p>GAN网络其实就是两个网络之间的博弈游戏，即网络和网络之间的对抗，其中一个网络G从随机噪声生成样本，而网络D则从来评判一个样本是真实数据的概率，即判别它是来自真实数据集的可能性大还是来自G生成的可能性大。</p>
<p>这样就形成了一个两个网络之间的博弈过程，那么理想状态下，G希望生成能以假乱真的样本，而D希望能有足够的能力分辨样本是生成的还是真实数据。这样的博弈最优的情况就是D(G(z))=0.5，这样就达到了一最优的状态。这样就说明D已经比较难以判断G生成的图片是否是真的。这样这个模型中的G网络就可以用来生成数据样本了，目的就达到了。用数学公式描述如下$$\underset Gmin\underset Dmax V(D,G)=E_{x_\tilde{} p_{data}}(x)[logD(x)]+E_{z\tilde{} p_z(z)}[log(1-D(G(z)))]$$我们可以简单解释下这个公式:</p>
<ul>
<li>首先x表示的就是真实样本，z表示随机生成的原始噪声，G(z)表示我们生成网络从噪声当中生成的图片，D(x)表示判别x是真实数据的概率（如果x就是真实的，那么我们希望D(x)接近1，反之希望接近0）。</li>
<li>G网络：它希望自己生成的样本越真实越好，即希望D(G(z))越大越好，那么上述公式的第二部分就会变小，整个式子V(D,G)也跟着变小。所以可以看到式子前面的符号为$\underset G min$。</li>
<li>D网络：它则是希望自己有足够强的能力能够有能力分辨真假样本，即D(x)应该大，D(G(z))应该小。那么log(D(x))会变大，log(1-D(G(z)))会变大，整个式子V(D,G)变大，所以式子前面符号为$\underset Dmax$</li>
</ul>
<p>也可以把上面的式子拆成两部分，一部分是优化D的一部分是优化G的，则对于G网络$$min\log(1-D(G(z)))$$对于D网络$$max\ (logD(x)+log(1-D(G(z))))$$这两个部分交替优化最终达到平衡。作者指出在每遍循环中同时优化D和G有可能导致过拟合，所以建议在k轮的D优化后再优化一遍G。</p>
<p>同时对于上述的对G网络的优化，由于刚开始G网络还很弱，因此log(1-D(G(z)))会非常小造成溢出，则算法会变得不稳定。作者建议把最小化log(1-D(G(z)))改为最大化log(D(G(z)))会使得算法在刚开始G网络还不是很强时更加稳定（但作者在给出的算法流程中并没有使用）。</p>
<h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h1><p><img src="/" class="lazyload" data-src="https://pic.downk.cc/item/5e8842a5504f4bcb04aa698a.png"  alt="">注意算法中第一个式子，即优化D的是ascending，因为要求最大化第一个式子，所以是加上梯度，而优化G的第二个式子就是一般的梯度下降，所以是减去梯度。</p>
<h1 id="一点想法"><a href="#一点想法" class="headerlink" title="一点想法"></a>一点想法</h1><ol>
<li><p>其实本文从数学角度很详细的论证了该算法的正确性以及收敛性，证明了在生成的样本和真实样本分布相同时，上述算法能收敛到最优，此时D(x)=$\frac{1}{2}$。具体的推导参考<a href="https://ninghaiywx.github.io/2020/04/18/Gan网络全局最优解推导/">另一篇博客</a>。</p>
</li>
<li><p>本文开创性的提出了一个新的思想，但没有给出具体的G网络和D网络的定义，文章中也指出虽然算法是可以收敛到最优点的，但是还取决于是否选取了合适的D和G。当然基于这个思想，已经有十分多的人提出了具体的D网络和G网络的实现并且有着很好的性能。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>深度学习</category>
        <category>论文</category>
        <category>2014年</category>
      </categories>
      <tags>
        <tag>论文</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：u-net</title>
    <url>/2020/04/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9Au-net/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>​    本论文介绍了一种U型网络结构，用于语义分割。它其实基于一种编码与解码的思想，可以有效的结合低分辨率的信息和高分辨率的信息，能够更好的分割图像边缘。它与FCN同一年提出，在思想上上也类似，但是u-net用了完全对称的结构，以及在拼接图像时用的不是像素的相加，而是通道的叠加，在我实际使用过程中发现比FCN-8有更好的分割精度，这得益于它的对称结构连接了更多的图像语义信息，而FCN-8则相对较少。本论文作者为<em>Olaf Ronneberger, Philipp Fischer, Thomas Brox</em></p>
<p><a href="https://arxiv.org/pdf/1505.04597.pdf" target="_blank" rel="noopener">原论文</a></p>
<h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p>​    本篇虽然给出了具体的网络结构，但这篇论文对我们更重要的影响是它使用的这种U型编码解码结构，至今的许多结构都是基于这种结构进行的完善和改进。原文中给出的网络结构如下<img src="/" class="lazyload" data-src="https://pic.downk.cc/item/5e996507c2a9a83be5805b8a.png"  alt="">    图例中也很明确的写出了每一步的操作是什么（其中卷积层的激活函数都使用的事ReLU），最终的输出就是基于每个像素的softmax分类。可以看到整个网络结构就是个U型的，左半部分相当于是编码部分，把原尺寸的图像进行特征提取压缩形成一个热图，然后再对其进行上采样的同时与原先的图像层进行拼接，有助于帮助恢复上采样中丢失的细节信息。</p>
<h1 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h1><h2 id="权重图"><a href="#权重图" class="headerlink" title="权重图"></a>权重图</h2><p>​    论文中给出了一个weight map的计算方法，它在训练之前就预先算好。它的作用就在于对于相连的物体有很好的分割作用，它给予相连物体之间的边界背景在损失函数中有极大的权重。下面是它的计算公式$$w(x) = w_c(x)+w_0.exp(-\frac{(d_z(x)+d_2(x))^2}{2\sigma^2})$$</p>
<p>$w_c$:是用于平衡类频率的权重图</p>
<p>$d_1$:表示当前像素到达最近边界单元格的距离</p>
<p>$d_2$:表示当前像素到达第二近边界单元格的距离</p>
<p>$w_0$和$\sigma$都是超参数。</p>
<p>（实际上我对这个权重图的方法的计算细节并不是特别清楚，网上搜寻后也没有人对这个方法有解答，在实际的实现中并没有使用它的这个方法。）</p>
<h2 id="overlap-tile"><a href="#overlap-tile" class="headerlink" title="overlap-tile"></a>overlap-tile</h2><p>这是作者论文中有一个令人有点困惑的地方。我的理解是，作者原文卷积操作并没有使用same卷积而是用了valid卷积，导致图像越卷越小并且丢失了一些边缘信息，那为何不用padding填充。作者的意思可能是输入图片非常的大，由于显存限制需要把一张图片分割后再进行拼接，使用overlap-tile的方法就能使得合并后的边缘更加合理。那么overlap-tile具体是怎么做，例如是左上角的边缘信息，那么我们就把它右边和下面的一部分图像做镜像填充到上面和左边再进行卷积，如下图<img src="/" class="lazyload" data-src="https://pic.downk.cc/item/5e996e69c2a9a83be58e3587.png"  alt="">黄色是卷积核卷积的部分，它的左边和上边都被右边和下边的镜像填充了（由于图像不大，这个方法我也没有进行尝试）。</p>
<h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><p>由于训练集不大以及增强网络的不变性和鲁棒性，需要使用一些增强数据的方式，文章是对细胞图像进行的分割，所以使用了弹性形变增强数据，这也符合细胞具有的生物学特性。实际使用过程中也可以根据实际情况使用其它的如平移、旋转等方式对进行图像增强。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>实验使用了tensorflow2中keras实现，利用了在imagenet上已经训练好的vgg-16网络中的前14层并设置不再更新这些层的参数。数据集利用了<a href="https://drive.google.com/file/d/0B0d9ZiqAgFkiOHR1NTJhWVJMNEU/view" target="_blank" rel="noopener">这里的数据</a>并且卷积层都使用了same卷积，下面就是网络主体的框架。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vgg16_model = tf.keras.applications.vgg16.VGG16(weights=<span class="string">'imagenet'</span>, include_top=<span class="literal">False</span>, input_tensor=keras.Input(shape=(<span class="number">320</span>, <span class="number">320</span>, <span class="number">3</span>)))</span><br><span class="line">vgg16_model.trainable = <span class="literal">False</span></span><br><span class="line">vgg16_model.summary()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">unet_model</span><span class="params">(tf.keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,n_class)</span>:</span></span><br><span class="line">      super().__init__()</span><br><span class="line">      self.n_class = n_class</span><br><span class="line">      self.vgg16_model = vgg16_model</span><br><span class="line">      self.conv1_1 = vgg16_model.layers[<span class="number">1</span>]</span><br><span class="line">      self.conv1_2 = vgg16_model.layers[<span class="number">2</span>]</span><br><span class="line">      self.pool1 = vgg16_model.layers[<span class="number">3</span>]</span><br><span class="line">     </span><br><span class="line">      self.conv2_1 = vgg16_model.layers[<span class="number">4</span>]</span><br><span class="line">      self.conv2_2 = vgg16_model.layers[<span class="number">5</span>]</span><br><span class="line">      self.pool2 = vgg16_model.layers[<span class="number">6</span>]</span><br><span class="line">        </span><br><span class="line">      self.conv3_1 = vgg16_model.layers[<span class="number">7</span>]</span><br><span class="line">      self.conv3_2 = vgg16_model.layers[<span class="number">8</span>]</span><br><span class="line">      self.conv3_3 = vgg16_model.layers[<span class="number">9</span>]</span><br><span class="line">      self.pool3 =  vgg16_model.layers[<span class="number">10</span>]</span><br><span class="line">       </span><br><span class="line">      self.conv4_1 = vgg16_model.layers[<span class="number">11</span>]</span><br><span class="line">      self.conv4_2 = vgg16_model.layers[<span class="number">12</span>]</span><br><span class="line">      self.conv4_3 = vgg16_model.layers[<span class="number">13</span>]</span><br><span class="line">      self.pool4 = vgg16_model.layers[<span class="number">14</span>]</span><br><span class="line">        </span><br><span class="line">      self.conv6 = Conv2D(<span class="number">1024</span>,(<span class="number">3</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">"same"</span>,activation=<span class="string">"relu"</span>)</span><br><span class="line">      self.conv7 = Conv2D(<span class="number">512</span>,(<span class="number">3</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">"same"</span>,activation=<span class="string">"relu"</span>)</span><br><span class="line">      self.conv_t1 = Conv2DTranspose(<span class="number">512</span>,(<span class="number">2</span>,<span class="number">2</span>),(<span class="number">2</span>,<span class="number">2</span>),padding=<span class="string">"same"</span>)</span><br><span class="line">      self.fuse_1 = Concatenate()</span><br><span class="line">      self.conv8 = Conv2D(<span class="number">512</span>,(<span class="number">3</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">"same"</span>,activation=<span class="string">"relu"</span>)</span><br><span class="line">      self.conv9 = Conv2D(<span class="number">256</span>,(<span class="number">3</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">"same"</span>,activation=<span class="string">"relu"</span>)</span><br><span class="line">      self.conv_t2 = Conv2DTranspose(<span class="number">256</span>,(<span class="number">2</span>,<span class="number">2</span>),(<span class="number">2</span>,<span class="number">2</span>),padding=<span class="string">"same"</span>)</span><br><span class="line">      self.fuse_2 = Concatenate()</span><br><span class="line">      self.conv10 = Conv2D(<span class="number">256</span>,(<span class="number">3</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">"same"</span>,activation=<span class="string">"relu"</span>)</span><br><span class="line">      self.conv11 = Conv2D(<span class="number">128</span>,(<span class="number">3</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">"same"</span>,activation=<span class="string">"relu"</span>)</span><br><span class="line">      self.conv_t3 = Conv2DTranspose(<span class="number">128</span>,(<span class="number">2</span>,<span class="number">2</span>),(<span class="number">2</span>,<span class="number">2</span>),padding=<span class="string">"same"</span>)</span><br><span class="line">      self.fuse_3 = Concatenate()</span><br><span class="line">      self.conv12 = Conv2D(<span class="number">128</span>,(<span class="number">3</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">"same"</span>,activation=<span class="string">"relu"</span>)</span><br><span class="line">      self.conv13 = Conv2D(<span class="number">64</span>,(<span class="number">3</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">"same"</span>,activation=<span class="string">"relu"</span>)</span><br><span class="line">      self.conv_t4 = Conv2DTranspose(<span class="number">64</span>,(<span class="number">2</span>,<span class="number">2</span>),(<span class="number">2</span>,<span class="number">2</span>),padding=<span class="string">"same"</span>)</span><br><span class="line">      self.fuse_4 = Concatenate()</span><br><span class="line">      self.conv14 = Conv2D(<span class="number">64</span>,(<span class="number">3</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">"same"</span>,activation=<span class="string">"relu"</span>)</span><br><span class="line">      self.conv15 = Conv2D(<span class="number">64</span>,(<span class="number">3</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">"same"</span>,activation=<span class="string">"relu"</span>)</span><br><span class="line">      self.conv16 = Conv2D(n_class,(<span class="number">1</span>,<span class="number">1</span>),(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">'same'</span>,activation=<span class="string">'softmax'</span>)</span><br><span class="line">    </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self,input)</span>:</span></span><br><span class="line"></span><br><span class="line">      x = self.conv1_1(input)</span><br><span class="line">      x_1 = self.conv1_2(x)</span><br><span class="line">      x = self.pool1(x_1)</span><br><span class="line">      x = self.conv2_1(x)</span><br><span class="line">      x_2 = self.conv2_2(x)</span><br><span class="line">      x = self.pool2(x_2)</span><br><span class="line">      x = self.conv3_1(x)</span><br><span class="line">      x = self.conv3_2(x)</span><br><span class="line">      x_3 = self.conv3_3(x)</span><br><span class="line">      x = self.pool3(x_3)</span><br><span class="line">      x = self.conv4_1(x)</span><br><span class="line">      x = self.conv4_2(x)</span><br><span class="line">      x_4 = self.conv4_3(x)</span><br><span class="line">      x = self.pool4(x_4)</span><br><span class="line"></span><br><span class="line">      x = self.conv6(x)</span><br><span class="line">      x = self.conv7(x)</span><br><span class="line">      x = self.conv_t1(x)</span><br><span class="line">      x = self.fuse_1([x,x_4])</span><br><span class="line"></span><br><span class="line">      x = self.conv8(x)</span><br><span class="line">      x = self.conv9(x)</span><br><span class="line">      x = self.conv_t2(x)</span><br><span class="line">      x = self.fuse_2([x,x_3])</span><br><span class="line"></span><br><span class="line">      x = self.conv10(x)</span><br><span class="line">      x = self.conv11(x)</span><br><span class="line">      x = self.conv_t3(x)</span><br><span class="line">      x = self.fuse_3([x,x_2])</span><br><span class="line"></span><br><span class="line">      x = self.conv12(x)</span><br><span class="line">      x = self.conv13(x)</span><br><span class="line">      x = self.conv_t4(x)</span><br><span class="line">      x = self.fuse_4([x,x_1])</span><br><span class="line"></span><br><span class="line">      x = self.conv14(x)</span><br><span class="line">      x = self.conv15(x)</span><br><span class="line">      x = self.conv16(x)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p>经过100轮训练，最终能在训练集上达到95%精度，如下图是在训练集上的测试。<img src="/" class="lazyload" data-src="https://pic.downk.cc/item/5e99720cc2a9a83be592ba36.png"  alt=""></p>
<p>​    但如果在测试集上就只有89%左右。由于训练集中人出现的少，测试集大多为人，可能在测试集效果不佳，可以把训练集和数据集加一起打乱后再分割成训练集和数据集再训练，以及利用数据增强的方法，应该会让模型的泛化能力更好。</p>
]]></content>
      <categories>
        <category>深度学习</category>
        <category>论文</category>
        <category>2015年</category>
      </categories>
      <tags>
        <tag>论文</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：deeplabv3+</title>
    <url>/2020/04/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9Adeeplabv3+/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>deeplabv3+是用于语义分割的deeplab的最新版本，其中加入了类似于U-net思想的解码器结构以及对于编码器中的Xception进行调整。该文章由谷歌团队发表，作者为<em>Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroffff, and Hartwig Adam</em></p>
<p><a href="https://arxiv.org/pdf/1802.02611v1.pdf" target="_blank" rel="noopener">原论文</a></p>
<h1 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h1><p><img src="/" class="lazyload" data-src="https://s1.ax1x.com/2020/04/24/JD8adO.png"  alt="JD8adO.png">)网络的整体架构如上图的(c)所示，它加入了编码-解码的思想，因为在编码的时候经过步长卷积或者池化层，细节信息有所丢失。所以在解码的时候与低层的信息相连接，可以有助于恢复图像边缘信息。更为详细的图如下所示<img src="/" class="lazyload" data-src="https://s1.ax1x.com/2020/04/25/JyQtfJ.png"  alt="JyQtfJ.png"></p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>这一部分对网络结构中用到的一些特殊方法和技巧做一些解释，包括空洞(膨胀)卷积，深度可分离卷积，编码器和解码器。</p>
<h2 id="空洞-膨胀-卷积"><a href="#空洞-膨胀-卷积" class="headerlink" title="空洞(膨胀)卷积"></a>空洞(膨胀)卷积</h2><p><img src="/" class="lazyload" data-src="https://s1.ax1x.com/2020/04/25/JyQz7T.png"  alt="JyQz7T.png"></p>
<p>可以看出来跟普通卷积的区别就在于在普通卷积中间注入了空洞，如上图其实就是一个5*5的卷积核和特诊图卷积，但是该卷积核有一些空洞，这就是rate=2的空洞卷积。也可以看出普通卷积就是rate=1的空洞卷积。这样的首要好处就是增大了感受野。</p>
<p>在二维情况下它的公式就是$$y[i]=\sum_kx[i+r.k]w[k]$$</p>
<p>r就是空洞卷积的rate。</p>
<p>当然空洞卷积也会带来问题，如果多次的使用rate为2的空洞卷积叠加，就会发现一直是不连续的像素参与了运算，虽然感受野增大了，但是损失了信息的连续性，这对于像素级的分类也是会损失很多精度的。如下图是3次rate=2的空洞卷积叠加<img src="/" class="lazyload" data-src="https://s1.ax1x.com/2020/04/25/Jy1Ns1.png"  alt="Jy1Ns1.png"></p>
<p>当然在deeplabv3+中并不涉及到这样的问题，因为它在ASPP中是分别使用了rate=1,6,12,18和一个pooling后的结果拼接后再进行卷积，可以参考网络架构中的图，这样的方法就结合了不同的感受野的特征图，理论上模型学习能力更好。</p>
<h2 id="深度可分离卷积"><a href="#深度可分离卷积" class="headerlink" title="深度可分离卷积"></a>深度可分离卷积</h2><p>深度可分离卷积最主要目的就是减少网络中的计算量，它是先用1*1卷积将特征图通道数增加，再对每个通道使用一个卷积核进行卷积，下图就是第二步骤的示意图<img src="/" class="lazyload" data-src="https://s1.ax1x.com/2020/04/25/Jy3fXR.png"  alt="Jy3fXR.png">这样的方法比起普通卷积在学习能力上会有一点点欠缺（不是特别明显），但是计算量少了非常多，可以减少为传统卷积的$\frac{1}{9}$-$\frac{1}{10}$左右。</p>
<p>深度可分离卷积的思想也可以用于空洞卷积上，如下图的(c)所示，可以较小计算量，使得模型更轻量。<img src="/" class="lazyload" data-src="https://s1.ax1x.com/2020/04/25/JyGMZt.png"  alt="JyGMZt.png"></p>
<h2 id="Deeplabv3作为编码器"><a href="#Deeplabv3作为编码器" class="headerlink" title="Deeplabv3作为编码器"></a>Deeplabv3作为编码器</h2><p>Deeplabv3+用了它的上一个版本Deeplabv3作为编码器，主要就是作为一个特征提取的主干网络，前面的下采样部分可以使用ResNet或者Xception等特征提取网络，可以下采样16倍，也可以只采样8倍，8倍效果会更好但是有更多的计算量。采样之后就紧接一个ASPP模块，用不同rate的空洞卷积获取到不同感受野和尺度的特征图，然后将他们拼接以后再做卷积。</p>
<h2 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h2><p>在之前的工作中，在编码器之后就直接使用16倍的双线性插值恢复原图像大小。但是这会使得物体边缘部分的分类出现问题，所以借用了类似U-net的思想，将ASPP输出的特征图上采样4倍后与主干网络中的低层特征进行拼接融合用于恢复边缘信息，具体操作可以对照网络架构部分的第二幅图。</p>
<h2 id="对Xception的调整"><a href="#对Xception的调整" class="headerlink" title="对Xception的调整"></a>对Xception的调整</h2><p>同时在用Xception做主干网络时，对它进行了调整使它更适合用于语义分割任务。具体的调整在：</p>
<ul>
<li>加深了Xception的深度（重复了16遍middle flow，原版只有8遍）。</li>
<li>所有的最大池化操作用带步长的深度可分离卷积替代。</li>
<li>在每个$3\times3$深度卷积后都加了BN层和ReLU激活函数。</li>
</ul>
<p>修改后的图如下<img src="/" class="lazyload" data-src="https://s1.ax1x.com/2020/04/25/JytPZn.png"  alt="JytPZn.png"></p>
<h1 id="我的实验"><a href="#我的实验" class="headerlink" title="我的实验"></a>我的实验</h1><p>作者在论文中也贴出了自己实现的<a href="https://github.com/tensorflow/models/tree/master/research/deeplab" target="_blank" rel="noopener">代码地址</a>)。作为个人学习我自己也简单做了一个实现。使用tensorflow2，主干网络用的就是原版的Xception并没有进行调整，同时ASPP模块部分使用普通卷积而没有用深度可分离卷积，数据集用的还是我在u-net文章中使用的(<a href="https://drive.google.com/file/d/0B0d9ZiqAgFkiOHR1NTJhWVJMNEU/view" target="_blank" rel="noopener">数据集</a>)。</p>
<h2 id="Xception"><a href="#Xception" class="headerlink" title="Xception"></a>Xception</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">back_bone = keras.applications.Xception(include_top=<span class="literal">False</span>,input_shape=(<span class="number">324</span>,<span class="number">324</span>,<span class="number">3</span>))</span><br><span class="line">layers_name = [</span><br><span class="line">        <span class="string">"block13_sepconv2_bn"</span>,</span><br><span class="line">        <span class="string">"block3_sepconv2_bn"</span></span><br><span class="line">]</span><br><span class="line">layers = [back_bone.get_layer(name).output <span class="keyword">for</span> name <span class="keyword">in</span> layers_name]</span><br><span class="line">down_stack = keras.Model(inputs=back_bone.input, outputs=layers)</span><br><span class="line">down_stack.trainable = <span class="literal">False</span></span><br></pre></td></tr></table></figure>

<p>用了tf2中提供的在Imagenet上训练过的Xception模型作为主干网络，也尝试过VGG-16和ResNet101，发现Xception效果最好。</p>
<h2 id="ASPP"><a href="#ASPP" class="headerlink" title="ASPP"></a>ASPP</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ASPP</span><span class="params">(X,f)</span>:</span></span><br><span class="line">  x_pool = AveragePooling2D()(X)</span><br><span class="line">  x_pool = Conv2D(f,padding=<span class="string">'same'</span>,kernel_size=<span class="number">1</span>)(x_pool)</span><br><span class="line">  x_pool = BatchNormalization(trainable=<span class="literal">False</span>)(x_pool)</span><br><span class="line">  x_pool = Activation(<span class="string">'relu'</span>)(x_pool)</span><br><span class="line">  x_pool = UpSampling2D(interpolation=<span class="string">'bilinear'</span>)(x_pool)</span><br><span class="line">  <span class="comment"># x_pool = Conv2DTranspose(f,(4,4),(2,2),padding='same')(x_pool)</span></span><br><span class="line"></span><br><span class="line">  x_1 = Conv2D(f,padding=<span class="string">'same'</span>,kernel_size=<span class="number">1</span>,dilation_rate=<span class="number">1</span>)(X)</span><br><span class="line">  x_1 = BatchNormalization()(x_1)</span><br><span class="line">  x_1 = Activation(<span class="string">'relu'</span>)(x_1)</span><br><span class="line"></span><br><span class="line">  x_6 = Conv2D(f,padding=<span class="string">'same'</span>,kernel_size=<span class="number">3</span>,dilation_rate=<span class="number">6</span>)(X)</span><br><span class="line">  x_6 = BatchNormalization(trainable=<span class="literal">False</span>)(x_6)</span><br><span class="line">  x_6 = Activation(<span class="string">'relu'</span>)(x_6)</span><br><span class="line"></span><br><span class="line">  x_12 = Conv2D(f,padding=<span class="string">'same'</span>,kernel_size=<span class="number">3</span>,dilation_rate=<span class="number">12</span>)(X)</span><br><span class="line">  x_12 = BatchNormalization(trainable=<span class="literal">False</span>)(x_12)</span><br><span class="line">  x_12 = Activation(<span class="string">'relu'</span>)(x_12)</span><br><span class="line"></span><br><span class="line">  x_18 = Conv2D(f,padding=<span class="string">'same'</span>,kernel_size=<span class="number">3</span>,dilation_rate=<span class="number">18</span>)(X)</span><br><span class="line">  x_18 = BatchNormalization(trainable=<span class="literal">False</span>)(x_18)</span><br><span class="line">  x_18 = Activation(<span class="string">'relu'</span>)(x_18)</span><br><span class="line"></span><br><span class="line">  x = Concatenate()([x_pool,x_1,x_6,x_12,x_18])</span><br><span class="line">  x = Conv2D(f,padding=<span class="string">'same'</span>,kernel_size=<span class="number">1</span>)(x)</span><br><span class="line">  x = BatchNormalization(trainable=<span class="literal">False</span>)(x)</span><br><span class="line">  x = Activation(<span class="string">'relu'</span>)(x)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p>上采样部分可以使用二次线性插值或者转置卷积。</p>
<h2 id="整体网络"><a href="#整体网络" class="headerlink" title="整体网络"></a>整体网络</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DeepLabv3plus</span><span class="params">(input_shape,n_class)</span>:</span></span><br><span class="line">  x_input = Input(input_shape)</span><br><span class="line">  x_padding = ZeroPadding2D(padding=(<span class="number">2</span>,<span class="number">2</span>))(x_input)</span><br><span class="line">  image_features = down_stack(x_padding)[<span class="number">0</span>]</span><br><span class="line">  x_a = ASPP(image_features,<span class="number">256</span>)</span><br><span class="line">  x_a = UpSampling2D((<span class="number">4</span>,<span class="number">4</span>),interpolation=<span class="string">'bilinear'</span>)(x_a)</span><br><span class="line">  <span class="comment"># x_a = Conv2DTranspose(256,(8,8),(4,4),padding='same')(x_a)</span></span><br><span class="line">  </span><br><span class="line">  x_b = down_stack(x_padding)[<span class="number">1</span>]</span><br><span class="line">  x_b = Conv2D(filters=<span class="number">48</span>,padding=<span class="string">'same'</span>,kernel_size=<span class="number">1</span>)(x_b)</span><br><span class="line">  x_b = BatchNormalization(trainable=<span class="literal">False</span>)(x_b)</span><br><span class="line">  x_b = Activation(<span class="string">'relu'</span>)(x_b)</span><br><span class="line"></span><br><span class="line">  x = Concatenate()([x_a,x_b])</span><br><span class="line">  x = Conv2D(filters=<span class="number">256</span>,kernel_size=<span class="number">3</span>,padding=<span class="string">'same'</span>)(x)</span><br><span class="line">  x = BatchNormalization(trainable=<span class="literal">False</span>)(x)</span><br><span class="line">  x = Activation(<span class="string">'relu'</span>)(x)</span><br><span class="line">  x = Conv2D(filters=<span class="number">256</span>,kernel_size=<span class="number">3</span>,padding=<span class="string">'same'</span>)(x)</span><br><span class="line">  x = BatchNormalization(trainable=<span class="literal">False</span>)(x)</span><br><span class="line">  x = Activation(<span class="string">'relu'</span>)(x)</span><br><span class="line">  x = Dropout(<span class="number">0.2</span>)(x)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># x = Conv2DTranspose(256,(8,8),(4,4),padding='same')(x)</span></span><br><span class="line">  x = UpSampling2D((<span class="number">4</span>,<span class="number">4</span>),interpolation=<span class="string">'bilinear'</span>)(x)</span><br><span class="line">  x = Conv2D(n_class,(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">'same'</span>,activation=<span class="string">'softmax'</span>)(x)</span><br><span class="line">  model = keras.Model(inputs=x_input,outputs=x)</span><br><span class="line">  <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>

<p>同样上采样部分可以使用二次线性插值或者转置卷积。</p>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p><img src="/" class="lazyload" data-src="https://s1.ax1x.com/2020/04/25/JyUYbd.png"  alt="JyUYbd.png"></p>
<p>以上是训练集的图片，最下面是网络分类的结果，可以看到几乎一致。在训练集上最高能达到96%的准确率还要高，但是在测试集上却在91%左右，可能由于图片不足以及训练集和测试集分布不一致导致的（训练集行人较少，测试集则都是行人，实际也是在行人等小物件分类出现不足）。但是总体来看还是可以看出deeplabv3+的实力还是不俗的。</p>
]]></content>
      <categories>
        <category>深度学习</category>
        <category>论文</category>
        <category>2018年</category>
      </categories>
      <tags>
        <tag>论文</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
</search>
